{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readme:\n",
    " - Always open the file before loading it to a dataframe and pay attention on how the data is represented in the file and then in the dataframe.\n",
    "\n",
    "We encourage you to explore more functionalities in 'Python for Data Analysis, 3E' by Wes McKinney, Chapter 6: 'Data Loading, Storage, and File Formats'.</br>\n",
    "Link: https://wesmckinney.com/book/accessing-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Intro to CSV files manipulation</b></h3>\n",
    "<p>\n",
    "For files with more complicated or fixed multicharacter delimiters, you will not be able to use the csv module. </br>\n",
    "In those cases, you’ll have to do the line splitting and other cleanup using the string’s split method or the regular expression method re.split. </br>\n",
    "Thankfully, pandas.read_csv is capable of doing almost anything you need if you pass the necessary options, so you only rarely will have to parse files by hand. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 1 </b></h3>\n",
    "<p>\n",
    "Create a dataframe out of file examples/ex1.csv and display it. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"examples/ex1.csv\")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 2 </b></h3>\n",
    "<p>\n",
    "Now create a dataframe ot of a similar csv file but with no header (examples/ex2.csv) so that it has default column names or names of your choice. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3      4\n",
      "0  1   2   3   4  hello\n",
      "1  5   6   7   8  world\n",
      "2  9  10  11  12    foo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"examples/ex2.csv\", header=None)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 3 </b></h3>\n",
    "<p>\n",
    "Create a hierarchical index out of key1, key2 in examples/csv_mindex.csv dataframe.</br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           value1  value2\n",
      "key1 key2                \n",
      "one  a          1       2\n",
      "     b          3       4\n",
      "     c          5       6\n",
      "     d          7       8\n",
      "two  a          9      10\n",
      "     b         11      12\n",
      "     c         13      14\n",
      "     d         15      16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"examples/csv_mindex.csv\", index_col=[\"key1\", \"key2\"])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 4 </b></h3>\n",
    "<p>\n",
    "What if you have a txt file with a white space delimiter where the number of white spaces may differ?</br>\n",
    "Convert file examples/ex3.txt to a dataframe using regex expression as a separator </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            A         B         C\n",
      "aaa -0.264438 -1.026059 -0.619500\n",
      "bbb  0.927272  0.302904 -0.032399\n",
      "ccc -0.264273 -0.386314 -0.217601\n",
      "ddd -0.871858 -0.348382  1.100491\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"examples/ex3.txt\", sep=r\"\\s+\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 5 </b></h3>\n",
    "<p>\n",
    "Convert file examples/ex4.csv to a dataframe by skipping rows 0, 2, 3: </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"examples/ex4.csv\", skiprows=[0, 2, 3])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 6 </b></h3>\n",
    "<p>\n",
    "How many missing values we have in file examples/ex5.csv? </br>\n",
    "1. Convert it to a dataframe and compare how the missing values are represented in the file and in the dataframe.</br>\n",
    "2. Then return a boolean dataframe showing if a value is missing using 'isna()' method. </br>\n",
    "\n",
    "Note: By default, pandas uses a set of commonly occurring sentinels, such as NA and NULL, you can add another custom value for missing val using 'na_values=[]' </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "   something  a   b     c   d message\n",
      "0       one  1   2   3.0   4     NaN\n",
      "1       two  5   6   NaN   8   world\n",
      "2     three  9  10  11.0  12     foo\n",
      "\n",
      "Missing Values (Boolean Mask):\n",
      "    something      a      b      c      d  message\n",
      "0      False  False  False  False  False     True\n",
      "1      False  False  False   True  False    False\n",
      "2      False  False  False  False  False    False\n",
      "\n",
      "Total missing values: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"examples/ex5.csv\")\n",
    "\n",
    "# Display the DataFrame to inspect how missing values appear\n",
    "print(\"DataFrame:\\n\", df)\n",
    "\n",
    "# 2. Show where values are missing\n",
    "missing_mask = df.isna()\n",
    "print(\"\\nMissing Values (Boolean Mask):\\n\", missing_mask)\n",
    "\n",
    "# 3. Count total missing values\n",
    "total_missing = missing_mask.sum().sum()\n",
    "print(f\"\\nTotal missing values: {total_missing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 7 </b></h3>\n",
    "<p>\n",
    "1. Convert file examples/ex5.csv to a dataframe using keep_default_na=False and analyze the result. </br>\n",
    "2. Then run isna() method on it and analyse the result - missing data is not recognied as such anymore. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with keep_default_na=False:\n",
      "   something  a   b   c   d message\n",
      "0       one  1   2   3   4      NA\n",
      "1       two  5   6       8   world\n",
      "2     three  9  10  11  12     foo\n",
      "\n",
      "Missing Values (Boolean Mask):\n",
      "    something      a      b      c      d  message\n",
      "0      False  False  False  False  False    False\n",
      "1      False  False  False  False  False    False\n",
      "2      False  False  False  False  False    False\n",
      "\n",
      "Total missing values: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the CSV with `keep_default_na=False`\n",
    "df = pd.read_csv(\"examples/ex5.csv\", keep_default_na=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"DataFrame with keep_default_na=False:\\n\", df)\n",
    "\n",
    "# 2. Run isna() to check for missing values\n",
    "missing_mask = df.isna()\n",
    "print(\"\\nMissing Values (Boolean Mask):\\n\", missing_mask)\n",
    "\n",
    "# 3. Count total missing values\n",
    "total_missing = missing_mask.sum().sum()\n",
    "print(f\"\\nTotal missing values: {total_missing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 8 </b></h3>\n",
    "<p>\n",
    "Convert file examples/ex5.csv to a dataframe so it returns 'NaN' for 'NA' values only, but keeps empty values as is </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with 'NA' as NaN and empty strings preserved:\n",
      "   something  a   b   c   d message\n",
      "0       one  1   2   3   4     NaN\n",
      "1       two  5   6       8   world\n",
      "2     three  9  10  11  12     foo\n",
      "\n",
      "Missing values (isna() result):\n",
      "    something      a      b      c      d  message\n",
      "0      False  False  False  False  False     True\n",
      "1      False  False  False  False  False    False\n",
      "2      False  False  False  False  False    False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV: treat only \"NA\" as NaN, keep empty strings as-is\n",
    "df = pd.read_csv(\"examples/ex5.csv\", keep_default_na=False, na_values=[\"NA\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"DataFrame with 'NA' as NaN and empty strings preserved:\\n\", df)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values (isna() result):\\n\", df.isna())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 9 </b></h3>\n",
    "<p>\n",
    "What is you want to assign NaN to the value '5' in column 'a', and assign NaN to the values '4', '8' in column 'd' by keeping 'NA' and empty values as is? </br>\n",
    "Use a dictionary to complete the task.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with selective NaNs:\n",
      "   something    a   b   c     d message\n",
      "0       one  1.0   2   3   NaN      NA\n",
      "1       two  NaN   6       NaN   world\n",
      "2     three  9.0  10  11  12.0     foo\n",
      "\n",
      "Missing values (isna() result):\n",
      "    something      a      b      c      d  message\n",
      "0      False  False  False  False   True    False\n",
      "1      False   True  False  False   True    False\n",
      "2      False  False  False  False  False    False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define column-specific NaN mappings\n",
    "na_dict = {\n",
    "    'a': ['5'],\n",
    "    'd': ['4', '8']\n",
    "}\n",
    "\n",
    "# Load CSV with custom missing values\n",
    "df = pd.read_csv(\"examples/ex5.csv\", na_values=na_dict, keep_default_na=False)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(\"DataFrame with selective NaNs:\\n\", df)\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values (isna() result):\\n\", df.isna())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 10 </b></h3>\n",
    "<p>\n",
    "1. Convert file examples/ex5.csv to a dataframe, and then back to csv file examples/out.csv.</br>\n",
    "2. Then print the dataframe to sys.stdout using pilcrow character '¶' as delimiter and representing missing values as 'NULL'.</br>\n",
    "3. Then print the dataframe to sys.stdout without the index and the header.</br>\n",
    "4. Then print the dataframe to sys.stdout without the index, choosing only columns 'something' and 'message'. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¶something¶a¶b¶c¶d¶message\n",
      "0¶one¶1¶2¶3.0¶4¶NULL\n",
      "1¶two¶5¶6¶NULL¶8¶world\n",
      "2¶three¶9¶10¶11.0¶12¶foo\n",
      "\n",
      "one,1,2,3.0,4,\n",
      "two,5,6,,8,world\n",
      "three,9,10,11.0,12,foo\n",
      "\n",
      "something,message\n",
      "one,\n",
      "two,world\n",
      "three,foo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# 1. Read file examples/ex5.csv into a dataframe\n",
    "df = pd.read_csv(\"examples/ex5.csv\")\n",
    "\n",
    "# Convert dataframe back to CSV file examples/out.csv\n",
    "df.to_csv(\"examples/out.csv\", index=True)\n",
    "\n",
    "# 2. Print dataframe to sys.stdout using pilcrow '¶' as delimiter and missing values as 'NULL'\n",
    "df.to_csv(sys.stdout, sep='¶', na_rep='NULL')\n",
    "\n",
    "print()  # Just to add a newline for clarity\n",
    "\n",
    "# 3. Print dataframe to sys.stdout without index and header\n",
    "df.to_csv(sys.stdout, index=False, header=False)\n",
    "\n",
    "print()  # Newline for clarity\n",
    "\n",
    "# 4. Print dataframe to sys.stdout without index, selecting only columns 'something' and 'message'\n",
    "df.to_csv(sys.stdout, index=False, columns=['something', 'message'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Intro to JSON files manipulation</b></h3>\n",
    "<p>\n",
    "JSON (short for JavaScript Object Notation) has become one of the standard formats for sending data by HTTP request between web browsers and other applications. It is a much more free-form data format than a tabular text form like CSV.</br>\n",
    "</br>\n",
    "<h3><b>Task 11 </b></h3>\n",
    "<p>\n",
    "1. Convert file examples/example.json to a dataframe and analyze how the data is represented.</br>\n",
    "2. Then return the dataframe to sys.stdout in the following format: [{\"a\":1,\"b\":2,\"c\":3},{\"a\":4,\"b\":5,\"c\":6},{\"a\":7,\"b\":8,\"c\":9}] </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrame:\n",
      "   a  b  c\n",
      "0  1  2  3\n",
      "1  4  5  6\n",
      "2  7  8  9\n",
      "[{\"a\":1,\"b\":2,\"c\":3},{\"a\":4,\"b\":5,\"c\":6},{\"a\":7,\"b\":8,\"c\":9}]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# 1. Read the JSON file into a DataFrame\n",
    "df = pd.read_json(\"examples/example.json\")\n",
    "print(\"Loaded DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# 2. Convert DataFrame to the required JSON format and print to sys.stdout\n",
    "df.to_json(sys.stdout, orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>XML and HTML: Web Scraping. </b></h3>\n",
    "<p>\n",
    "Libraries include lxml, Beautiful Soup, and html5lib. While lxml is comparatively much faster in general, the other libraries can better handle malformed HTML or XML files.</br>\n",
    "pandas has a built-in function, pandas.read_html, which uses all of these libraries to automatically parse tables out of HTML files as DataFrame objects. </br>\n",
    "First, you must install some additional libraries used by read_html:</br>\n",
    "conda install lxml beautifulsoup4 html5lib   </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\gergana.boycheva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (5.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3><b>Task 12 </b></h3><p>\n",
    "The pandas.read_html function has a number of options, but by default it searches for and attempts to parse all tabular data contained within 'table' tags. </br>\n",
    "What data type below code will return? Run it and analyze the result.\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n",
      "                      Bank Name             City  ST   CERT  \\\n",
      "0                   Allied Bank         Mulberry  AR     91   \n",
      "1  The Woodbury Banking Company         Woodbury  GA  11297   \n",
      "2        First CornerStone Bank  King of Prussia  PA  35312   \n",
      "3            Trust Company Bank          Memphis  TN   9956   \n",
      "4    North Milwaukee State Bank        Milwaukee  WI  20364   \n",
      "\n",
      "                 Acquiring Institution        Closing Date       Updated Date  \n",
      "0                         Today's Bank  September 23, 2016  November 17, 2016  \n",
      "1                          United Bank     August 19, 2016  November 17, 2016  \n",
      "2  First-Citizens Bank & Trust Company         May 6, 2016  September 6, 2016  \n",
      "3           The Bank of Fayette County      April 29, 2016  September 6, 2016  \n",
      "4  First-Citizens Bank & Trust Company      March 11, 2016      June 16, 2016  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read all tables\n",
    "df_list = pd.read_html('examples/fdic_failed_bank_list.html')\n",
    "\n",
    "# Check the type\n",
    "print(type(df_list))            # <class 'list'>\n",
    "\n",
    "# Check how many tables were found\n",
    "print(len(df_list))             # Number of tables found\n",
    "\n",
    "# Display the first table\n",
    "print(df_list[0].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 13 </b></h3>\n",
    "<p>\n",
    "Now run below and analyze the result. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_list[0]\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 14</b></h3>\n",
    "<p>\n",
    "Parsing XML with lxml.objectify. </br> \n",
    "Run below code and analyze what it does.</br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            AGENCY_NAME                        INDICATOR_NAME  \\\n",
      "0  Metro-North Railroad  On-Time Performance (West of Hudson)   \n",
      "1  Metro-North Railroad  On-Time Performance (West of Hudson)   \n",
      "2  Metro-North Railroad  On-Time Performance (West of Hudson)   \n",
      "3  Metro-North Railroad  On-Time Performance (West of Hudson)   \n",
      "4  Metro-North Railroad  On-Time Performance (West of Hudson)   \n",
      "\n",
      "                                         DESCRIPTION  PERIOD_YEAR  \\\n",
      "0  Percent of commuter trains that arrive at thei...         2008   \n",
      "1  Percent of commuter trains that arrive at thei...         2008   \n",
      "2  Percent of commuter trains that arrive at thei...         2008   \n",
      "3  Percent of commuter trains that arrive at thei...         2008   \n",
      "4  Percent of commuter trains that arrive at thei...         2008   \n",
      "\n",
      "   PERIOD_MONTH            CATEGORY FREQUENCY INDICATOR_UNIT YTD_TARGET  \\\n",
      "0             1  Service Indicators         M              %       95.0   \n",
      "1             2  Service Indicators         M              %       95.0   \n",
      "2             3  Service Indicators         M              %       95.0   \n",
      "3             4  Service Indicators         M              %       95.0   \n",
      "4             5  Service Indicators         M              %       95.0   \n",
      "\n",
      "  YTD_ACTUAL MONTHLY_TARGET MONTHLY_ACTUAL  \n",
      "0       96.9           95.0           96.9  \n",
      "1       96.0           95.0           95.0  \n",
      "2       96.3           95.0           96.9  \n",
      "3       96.8           95.0           98.3  \n",
      "4       96.6           95.0           95.8  \n"
     ]
    }
   ],
   "source": [
    "from lxml import objectify\n",
    "\n",
    "path = \"datasets/mta_perf/Performance_MNR.xml\"\n",
    "with open(path) as f:\n",
    "    parsed = objectify.parse(f)\n",
    "root = parsed.getroot()\n",
    "data = []\n",
    "skip_fields = [\"PARENT_SEQ\", \"INDICATOR_SEQ\", \"DESIRED_CHANGE\", \"DECIMAL_PLACES\"]\n",
    "for etl in root.INDICATOR:\n",
    "    el_data = {}\n",
    "    for child in etl.getchildren():\n",
    "        if child.tag in skip_fields:\n",
    "            continue \n",
    "        el_data[child.tag] = child.pyval \n",
    "    data.append(el_data)\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 15 </b></h3>\n",
    "<p>\n",
    "With pandas.read_xml all the lines of the above code with lxml library can be considerably reduced. </br>\n",
    "Convert the file from the previous task to a dataframe with one line of code using pandas.read_xml().\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INDICATOR_SEQ  PARENT_SEQ           AGENCY_NAME  \\\n",
      "0          28445         NaN  Metro-North Railroad   \n",
      "1          28445         NaN  Metro-North Railroad   \n",
      "2          28445         NaN  Metro-North Railroad   \n",
      "3          28445         NaN  Metro-North Railroad   \n",
      "4          28445         NaN  Metro-North Railroad   \n",
      "\n",
      "                         INDICATOR_NAME  \\\n",
      "0  On-Time Performance (West of Hudson)   \n",
      "1  On-Time Performance (West of Hudson)   \n",
      "2  On-Time Performance (West of Hudson)   \n",
      "3  On-Time Performance (West of Hudson)   \n",
      "4  On-Time Performance (West of Hudson)   \n",
      "\n",
      "                                         DESCRIPTION  PERIOD_YEAR  \\\n",
      "0  Percent of commuter trains that arrive at thei...         2008   \n",
      "1  Percent of commuter trains that arrive at thei...         2008   \n",
      "2  Percent of commuter trains that arrive at thei...         2008   \n",
      "3  Percent of commuter trains that arrive at thei...         2008   \n",
      "4  Percent of commuter trains that arrive at thei...         2008   \n",
      "\n",
      "   PERIOD_MONTH            CATEGORY FREQUENCY DESIRED_CHANGE INDICATOR_UNIT  \\\n",
      "0             1  Service Indicators         M              U              %   \n",
      "1             2  Service Indicators         M              U              %   \n",
      "2             3  Service Indicators         M              U              %   \n",
      "3             4  Service Indicators         M              U              %   \n",
      "4             5  Service Indicators         M              U              %   \n",
      "\n",
      "   DECIMAL_PLACES YTD_TARGET YTD_ACTUAL MONTHLY_TARGET MONTHLY_ACTUAL  \n",
      "0               1      95.00      96.90          95.00          96.90  \n",
      "1               1      95.00      96.00          95.00          95.00  \n",
      "2               1      95.00      96.30          95.00          96.90  \n",
      "3               1      95.00      96.80          95.00          98.30  \n",
      "4               1      95.00      96.60          95.00          95.80  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load XML file with one line\n",
    "df = pd.read_xml(\"datasets/mta_perf/Performance_MNR.xml\", xpath=\".//INDICATOR\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Reading Microsoft Excel Files </b></h3>\n",
    "<p>\n",
    "\n",
    "pandas also supports reading tabular data stored in Excel 2003 (and higher) files using either the pandas.ExcelFile class or pandas.read_excel function. </br>\n",
    "Internally, these tools use the add-on packages xlrd and openpyxl to read old-style XLS and newer XLSX files, respectively. These must be installed separately from pandas using pip or conda.</br></br>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\gergana.boycheva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\gergana.boycheva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openpyxl) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 16 </b></h3>\n",
    "1. Convert file examples/ex1.xlsx to a pandas parsable object using pandas.ExcelFile and display its type().</br>\n",
    "2. Then display the sheet names.</br>\n",
    "3. Parse the object and create a dataframe out of Sheet1 indicating that the 1st column is an index column. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.excel._base.ExcelFile'>\n",
      "['Sheet1']\n",
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the Excel file as an ExcelFile object\n",
    "xlsx = pd.ExcelFile(\"examples/ex1.xlsx\")\n",
    "print(type(xlsx))  # <class 'pandas.io.excel._base.ExcelFile'>\n",
    "\n",
    "# 2. Display the sheet names\n",
    "print(xlsx.sheet_names)\n",
    "\n",
    "# 3. Parse the \"Sheet1\" sheet, using the first column as the index\n",
    "df = xlsx.parse(\"Sheet1\", index_col=0)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 17 </b></h3>\n",
    "<p>\n",
    "If you are reading multiple sheets in a file, then it is faster to create the pandas.ExcelFile.</br>\n",
    "But you can also simply pass the filename to pandas.read_excel - try this out. </br>\n",
    "Then write the dataframe to file examples/ex2.xlsx by creating a sheet named 'test' </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file directly with read_excel (no ExcelFile used here)\n",
    "df = pd.read_excel(\"examples/ex1.xlsx\", sheet_name=\"Sheet1\", index_col=0)\n",
    "print(df)\n",
    "\n",
    "# Write the DataFrame to a new Excel file with a custom sheet name\n",
    "df.to_excel(\"examples/ex2.xlsx\", sheet_name=\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Interacting with Web APIs </b></h3>\n",
    "<p>\n",
    "Many websites have public APIs providing data feeds via JSON or some other format. There are a number of ways to access these APIs from Python.</br>\n",
    "One method that we recommend is the requests package, which can be installed with pip or conda.</br>\n",
    "Install and import 'requests' module.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl (105 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------- ----------------------- 2/5 [charset-normalizer]\n",
      "   ---------------------------------------- 5/5 [requests]\n",
      "\n",
      "Successfully installed certifi-2025.4.26 charset-normalizer-3.4.2 idna-3.10 requests-2.32.3 urllib3-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 18 </b></h3>\n",
    "Use the requests.get function to fetch the data from url 'https://api.github.com/repos/pandas-dev/pandas/issues' </br>\n",
    "Check the status code of the response by calling raise_for_status method.</br>\n",
    "Note: It's a good practice to always call raise_for_status after using requests.get to check for HTTP errors.</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "{'url': 'https://api.github.com/repos/pandas-dev/pandas/issues/61470', 'repository_url': 'https://api.github.com/repos/pandas-dev/pandas', 'labels_url': 'https://api.github.com/repos/pandas-dev/pandas/issues/61470/labels{/name}', 'comments_url': 'https://api.github.com/repos/pandas-dev/pandas/issues/61470/comments', 'events_url': 'https://api.github.com/repos/pandas-dev/pandas/issues/61470/events', 'html_url': 'https://github.com/pandas-dev/pandas/pull/61470', 'id': 3079915886, 'node_id': 'PR_kwDOAA0YD86XC5hP', 'number': 61470, 'title': 'DOC: Restructure and expand UDF page', 'user': {'login': 'datapythonista', 'id': 10058240, 'node_id': 'MDQ6VXNlcjEwMDU4MjQw', 'avatar_url': 'https://avatars.githubusercontent.com/u/10058240?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/datapythonista', 'html_url': 'https://github.com/datapythonista', 'followers_url': 'https://api.github.com/users/datapythonista/followers', 'following_url': 'https://api.github.com/users/datapythonista/following{/other_user}', 'gists_url': 'https://api.github.com/users/datapythonista/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/datapythonista/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/datapythonista/subscriptions', 'organizations_url': 'https://api.github.com/users/datapythonista/orgs', 'repos_url': 'https://api.github.com/users/datapythonista/repos', 'events_url': 'https://api.github.com/users/datapythonista/events{/privacy}', 'received_events_url': 'https://api.github.com/users/datapythonista/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OTk=', 'url': 'https://api.github.com/repos/pandas-dev/pandas/labels/Docs', 'name': 'Docs', 'color': '3465A4', 'default': False, 'description': None}, {'id': 697792067, 'node_id': 'MDU6TGFiZWw2OTc3OTIwNjc=', 'url': 'https://api.github.com/repos/pandas-dev/pandas/labels/Apply', 'name': 'Apply', 'color': 'fbca04', 'default': False, 'description': 'Apply, Aggregate, Transform, Map'}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2025-05-21T11:34:50Z', 'updated_at': '2025-05-21T11:34:50Z', 'closed_at': None, 'author_association': 'MEMBER', 'type': None, 'active_lock_reason': None, 'draft': False, 'pull_request': {'url': 'https://api.github.com/repos/pandas-dev/pandas/pulls/61470', 'html_url': 'https://github.com/pandas-dev/pandas/pull/61470', 'diff_url': 'https://github.com/pandas-dev/pandas/pull/61470.diff', 'patch_url': 'https://github.com/pandas-dev/pandas/pull/61470.patch', 'merged_at': None}, 'body': 'I changed the order in which the methods are presented,both in the table and in the sections, to be:\\r\\n\\r\\n- map\\r\\n- apply\\r\\n- pipe\\r\\n- filter\\r\\n- agg\\r\\n- transform\\r\\n\\r\\nI find it easier to explain them in this order.\\r\\n\\r\\nAnd I expanded the `map` and `apply` sections with examples and a bit more of information.\\r\\n\\r\\n@arthurlw @rhshadrach do you mind having a look?', 'closed_by': None, 'reactions': {'url': 'https://api.github.com/repos/pandas-dev/pandas/issues/61470/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/pandas-dev/pandas/issues/61470/timeline', 'performed_via_github_app': None, 'state_reason': None}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL to fetch issues from the pandas GitHub repo\n",
    "url = \"https://api.github.com/repos/pandas-dev/pandas/issues\"\n",
    "\n",
    "# Make the GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check for HTTP errors\n",
    "response.raise_for_status()  # Will raise an HTTPError if the status is 4xx or 5xx\n",
    "\n",
    "# Print success message and preview of JSON\n",
    "print(\"Request successful!\")\n",
    "issues = response.json()\n",
    "print(issues[0])  # Print the first issue for preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 19 </b></h3>\n",
    "<p>\n",
    "1. Use 'json' method to return a Python object containing the parsed JSON data as a dictionary or list (depending on what JSON is returned). </br>\n",
    "2. Convert the object to a dataframe  </br>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  \\\n",
      "0  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "1  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "2  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "3  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "4  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "\n",
      "                                   repository_url  \\\n",
      "0  https://api.github.com/repos/pandas-dev/pandas   \n",
      "1  https://api.github.com/repos/pandas-dev/pandas   \n",
      "2  https://api.github.com/repos/pandas-dev/pandas   \n",
      "3  https://api.github.com/repos/pandas-dev/pandas   \n",
      "4  https://api.github.com/repos/pandas-dev/pandas   \n",
      "\n",
      "                                          labels_url  \\\n",
      "0  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "1  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "2  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "3  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "4  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "\n",
      "                                        comments_url  \\\n",
      "0  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "1  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "2  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "3  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "4  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "\n",
      "                                          events_url  \\\n",
      "0  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "1  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "2  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "3  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "4  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "\n",
      "                                            html_url          id  \\\n",
      "0    https://github.com/pandas-dev/pandas/pull/61470  3079915886   \n",
      "1  https://github.com/pandas-dev/pandas/issues/61469  3079474716   \n",
      "2    https://github.com/pandas-dev/pandas/pull/61468  3078357968   \n",
      "3    https://github.com/pandas-dev/pandas/pull/61467  3078337779   \n",
      "4  https://github.com/pandas-dev/pandas/issues/61466  3078154773   \n",
      "\n",
      "               node_id  number  \\\n",
      "0  PR_kwDOAA0YD86XC5hP   61470   \n",
      "1   I_kwDOAA0YD863jQ4c   61469   \n",
      "2  PR_kwDOAA0YD86W9ncJ   61468   \n",
      "3  PR_kwDOAA0YD86W9jGU   61467   \n",
      "4   I_kwDOAA0YD863eOoV   61466   \n",
      "\n",
      "                                               title  ... active_lock_reason  \\\n",
      "0               DOC: Restructure and expand UDF page  ...               None   \n",
      "1  BUG: pandas.pivot_table margins, dropna and ob...  ...               None   \n",
      "2                             ENH: Implement PDEP-17  ...               None   \n",
      "3  ENH: Support third-party execution engines in ...  ...               None   \n",
      "4  BUG: Series.str.isdigit with pyarrow dtype doe...  ...               None   \n",
      "\n",
      "   draft                                       pull_request  \\\n",
      "0  False  {'url': 'https://api.github.com/repos/pandas-d...   \n",
      "1    NaN                                                NaN   \n",
      "2  False  {'url': 'https://api.github.com/repos/pandas-d...   \n",
      "3  False  {'url': 'https://api.github.com/repos/pandas-d...   \n",
      "4    NaN                                                NaN   \n",
      "\n",
      "                                                body closed_by  \\\n",
      "0  I changed the order in which the methods are p...      None   \n",
      "1  ### Pandas version checks\\n\\n- [x] I have chec...      None   \n",
      "2  - [ ] closes #xxxx (Replace xxxx with the GitH...      None   \n",
      "3  - [X] xref #61125\\r\\n- [X] [Tests added and pa...      None   \n",
      "4  ### Pandas version checks\\n\\n- [x] I have chec...      None   \n",
      "\n",
      "                                           reactions  \\\n",
      "0  {'url': 'https://api.github.com/repos/pandas-d...   \n",
      "1  {'url': 'https://api.github.com/repos/pandas-d...   \n",
      "2  {'url': 'https://api.github.com/repos/pandas-d...   \n",
      "3  {'url': 'https://api.github.com/repos/pandas-d...   \n",
      "4  {'url': 'https://api.github.com/repos/pandas-d...   \n",
      "\n",
      "                                        timeline_url  \\\n",
      "0  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "1  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "2  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "3  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "4  https://api.github.com/repos/pandas-dev/pandas...   \n",
      "\n",
      "   performed_via_github_app state_reason  \\\n",
      "0                      None         None   \n",
      "1                      None         None   \n",
      "2                      None         None   \n",
      "3                      None         None   \n",
      "4                      None         None   \n",
      "\n",
      "                                  sub_issues_summary  \n",
      "0                                                NaN  \n",
      "1  {'total': 0, 'completed': 0, 'percent_complete...  \n",
      "2                                                NaN  \n",
      "3                                                NaN  \n",
      "4  {'total': 0, 'completed': 0, 'percent_complete...  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Make the request and parse JSON into a Python object\n",
    "url = \"https://api.github.com/repos/pandas-dev/pandas/issues\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Ensure request succeeded\n",
    "\n",
    "# Use the .json() method to get a list of dictionaries (issues)\n",
    "issues_json = response.json()\n",
    "\n",
    "# Step 2: Convert JSON object to a pandas DataFrame\n",
    "issues_df = pd.DataFrame(issues_json)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(issues_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Interacting with Databases </b></h3>\n",
    "<p>\n",
    "Import Python’s built-in sqlite3 driver </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 20 </b></h3>\n",
    "<p>\n",
    "Create a SQLite3 table using below connection to execute the query. </br>\n",
    "Do not forget to commit your changes.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "query = \"\"\"CREATE TABLE if not exists test\n",
    "    (a VARCHAR(20), b VARCHAR(20),\n",
    "     c REAL,        d INTEGER\n",
    "    );\"\"\"\n",
    "\n",
    "con = sqlite3.connect(\"mydata.sqlite\")\n",
    "cursor = con.cursor()\n",
    "\n",
    "cursor.execute(query)\n",
    "con.commit()\n",
    "\n",
    "cursor.close()\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 21</b></h3>\n",
    "<p>\n",
    "Using below data, execute the insert statement and commit the changes.</br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "con = sqlite3.connect(\"mydata.sqlite\")\n",
    "cursor = con.cursor()\n",
    "\n",
    "# Data to insert\n",
    "data = [\n",
    "    (\"Atlanta\", \"Georgia\", 1.25, 6),\n",
    "    (\"Tallahassee\", \"Florida\", 2.6, 3),\n",
    "    (\"Sacramento\", \"California\", 1.7, 5)\n",
    "]\n",
    "\n",
    "# Insert statement\n",
    "stmt = \"INSERT INTO test VALUES (?, ?, ?, ?)\"\n",
    "\n",
    "# Execute insert for each row in data\n",
    "cursor.executemany(stmt, data)\n",
    "\n",
    "# Commit changes\n",
    "con.commit()\n",
    "\n",
    "# Close cursor and connection\n",
    "cursor.close()\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 22 </b></h3>\n",
    "<p>\n",
    "Now select all from the newly created table and display the result. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Atlanta', 'Georgia', 1.25, 6)\n",
      "('Tallahassee', 'Florida', 2.6, 3)\n",
      "('Sacramento', 'California', 1.7, 5)\n",
      "('Atlanta', 'Georgia', 1.25, 6)\n",
      "('Tallahassee', 'Florida', 2.6, 3)\n",
      "('Sacramento', 'California', 1.7, 5)\n",
      "('Atlanta', 'Georgia', 1.25, 6)\n",
      "('Tallahassee', 'Florida', 2.6, 3)\n",
      "('Sacramento', 'California', 1.7, 5)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "con = sqlite3.connect(\"mydata.sqlite\")\n",
    "cursor = con.cursor()\n",
    "\n",
    "# Execute the SELECT query\n",
    "cursor.execute(\"SELECT * FROM test\")\n",
    "\n",
    "# Fetch all results\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Display the results\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close cursor and connection\n",
    "cursor.close()\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 23 </b></h3>\n",
    "<p>\n",
    "Convert the result from the previous task to a dataframe, providing also the table column names. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             a           b     c  d\n",
      "0      Atlanta     Georgia  1.25  6\n",
      "1  Tallahassee     Florida  2.60  3\n",
      "2   Sacramento  California  1.70  5\n",
      "3      Atlanta     Georgia  1.25  6\n",
      "4  Tallahassee     Florida  2.60  3\n",
      "5   Sacramento  California  1.70  5\n",
      "6      Atlanta     Georgia  1.25  6\n",
      "7  Tallahassee     Florida  2.60  3\n",
      "8   Sacramento  California  1.70  5\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the SQLite database\n",
    "con = sqlite3.connect(\"mydata.sqlite\")\n",
    "\n",
    "# Use pandas.read_sql_query to directly get DataFrame including columns\n",
    "df = pd.read_sql_query(\"SELECT * FROM test\", con)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the connection\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>\n",
    "How to achive the same, but with fewer lines of code? </br>\n",
    "This is quite a bit of munging that you’d rather not repeat each time you query the database. The SQLAlchemy project is a popular Python SQL toolkit that abstracts away many of the common differences between SQL databases.  </br>\n",
    "pandas has a read_sql function that enables you to read data easily from a general SQLAlchemy connection.  </br>\n",
    "You can install SQLAlchemy with conda. </br>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\gergana.boycheva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.0.41)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\gergana.boycheva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy) (3.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\gergana.boycheva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy) (4.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Import sqlalchemy </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sqla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 24 </b></h3>\n",
    "<p>\n",
    "Create a dataframe using pandas.read_sql() and selecting all data from below db using one line of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             a           b     c  d\n",
      "0      Atlanta     Georgia  1.25  6\n",
      "1  Tallahassee     Florida  2.60  3\n",
      "2   Sacramento  California  1.70  5\n",
      "3      Atlanta     Georgia  1.25  6\n",
      "4  Tallahassee     Florida  2.60  3\n",
      "5   Sacramento  California  1.70  5\n",
      "6      Atlanta     Georgia  1.25  6\n",
      "7  Tallahassee     Florida  2.60  3\n",
      "8   Sacramento  California  1.70  5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sqla\n",
    "\n",
    "db = sqla.create_engine(\"sqlite:///mydata.sqlite\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM test\", db)\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

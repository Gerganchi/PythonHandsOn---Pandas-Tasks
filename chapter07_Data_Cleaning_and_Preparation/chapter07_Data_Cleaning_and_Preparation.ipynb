{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readme:\n",
    "\n",
    "\n",
    "We encourage you to explore more functionalities in 'Python for Data Analysis, 3E' by Wes McKinney, Chapter 7: 'Data Cleaning and Preparation'.</br>\n",
    "Link: https://wesmckinney.com/book/data-cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3><b>Task 1 </b></h3>\n",
    "\n",
    "1. Create a Series from list [\"aardvark\", np.nan, None, \"avocado\"] and observe how values 'np.nan' and 'None' are represented.  </br>\n",
    "2. What is the data type of the Series? </br>\n",
    "3. Then identify which values are considered 'NA' (Not available) by running isna() method. </br>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Series:\n",
      "0    aardvark\n",
      "1         NaN\n",
      "2        None\n",
      "3     avocado\n",
      "dtype: object\n",
      "\n",
      "Data type of the Series:\n",
      "object\n",
      "\n",
      "Missing values (NA) using isna():\n",
      "0    False\n",
      "1     True\n",
      "2     True\n",
      "3    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Create the Series\n",
    "data = pd.Series([\"aardvark\", np.nan, None, \"avocado\"])\n",
    "print(\"Original Series:\")\n",
    "print(data)\n",
    "\n",
    "# 2. Data type of the Series\n",
    "print(\"\\nData type of the Series:\")\n",
    "print(data.dtype)\n",
    "\n",
    "# 3. Identify NA values\n",
    "print(\"\\nMissing values (NA) using isna():\")\n",
    "print(data.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 2 </b></h3>\n",
    "<p>\n",
    "For data with float64 dtype, pandas uses the floating-point value NaN (Not a Number) to represent missing data.</br>\n",
    "Create a Series from list [1, 2, None] by specifying data type as 'float64' - how the 'None' value will be represented here? Compare it with 'None' representation in the previous task. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series with float64 dtype:\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    NaN\n",
      "dtype: float64\n",
      "\n",
      "Missing values (NA) using isna():\n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "dtype: bool\n",
      "\n",
      "Data type of the Series:\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create Series with float64 dtype\n",
    "s = pd.Series([1, 2, None], dtype=\"float64\")\n",
    "print(\"Series with float64 dtype:\")\n",
    "print(s)\n",
    "\n",
    "# Check which values are missing\n",
    "print(\"\\nMissing values (NA) using isna():\")\n",
    "print(s.isna())\n",
    "\n",
    "# Data type of the Series\n",
    "print(\"\\nData type of the Series:\")\n",
    "print(s.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 3 </b></h3>\n",
    "<p>\n",
    "Create a Series from list [1, np.nan, 3.5, np.nan, 7] and drop the missing values. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Series:\n",
      "0    1.0\n",
      "1    NaN\n",
      "2    3.5\n",
      "3    NaN\n",
      "4    7.0\n",
      "dtype: float64\n",
      "\n",
      "Series after dropping missing values:\n",
      "0    1.0\n",
      "2    3.5\n",
      "4    7.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create the Series\n",
    "s = pd.Series([1, np.nan, 3.5, np.nan, 7])\n",
    "\n",
    "# Drop missing values\n",
    "s_dropped = s.dropna()\n",
    "\n",
    "# Print the result\n",
    "print(\"Original Series:\")\n",
    "print(s)\n",
    "print(\"\\nSeries after dropping missing values:\")\n",
    "print(s_dropped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 4 </b></h3>\n",
    "<p>\n",
    "1. Create a dataframe out of nested list [[1., 6.5, 3.], [1., np.nan, np.nan], [np.nan, np.nan, np.nan], [np.nan, 6.5, 3.]].</br>\n",
    "2. Run dropna() method - will it drop a whole row containing at least one missing value or a row where all values are missing? </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     0    1    2\n",
      "0  1.0  6.5  3.0\n",
      "1  1.0  NaN  NaN\n",
      "2  NaN  NaN  NaN\n",
      "3  NaN  6.5  3.0\n",
      "\n",
      "DataFrame after dropna():\n",
      "     0    1    2\n",
      "0  1.0  6.5  3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create the DataFrame\n",
    "df = pd.DataFrame([\n",
    "    [1., 6.5, 3.],\n",
    "    [1., np.nan, np.nan],\n",
    "    [np.nan, np.nan, np.nan],\n",
    "    [np.nan, 6.5, 3.]\n",
    "])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# 2. Drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "\n",
    "print(\"\\nDataFrame after dropna():\")\n",
    "print(df_dropped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 5 </b></h3>\n",
    "<p>\n",
    "Rewrite the dropna() method so it drops only the rows where ALL values are missing.</br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     0    1    2\n",
      "0  1.0  6.5  3.0\n",
      "1  1.0  NaN  NaN\n",
      "2  NaN  NaN  NaN\n",
      "3  NaN  6.5  3.0\n",
      "\n",
      "DataFrame after dropping rows where all values are missing:\n",
      "     0    1    2\n",
      "0  1.0  6.5  3.0\n",
      "1  1.0  NaN  NaN\n",
      "3  NaN  6.5  3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Original DataFrame\n",
    "df = pd.DataFrame([\n",
    "    [1., 6.5, 3.],\n",
    "    [1., np.nan, np.nan],\n",
    "    [np.nan, np.nan, np.nan],\n",
    "    [np.nan, 6.5, 3.]\n",
    "])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Drop only rows where all values are NaN\n",
    "df_cleaned = df.dropna(how='all')\n",
    "\n",
    "print(\"\\nDataFrame after dropping rows where all values are missing:\")\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 6 </b></h3>\n",
    "<p>\n",
    "1. Add column indexed as 3 where all values are NA. </br>\n",
    "2. Drop the COLUMN where ALL values are NA.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with new all-NA column:\n",
      "     0    1    2   3\n",
      "0  1.0  6.5  3.0 NaN\n",
      "1  1.0  NaN  NaN NaN\n",
      "2  NaN  NaN  NaN NaN\n",
      "3  NaN  6.5  3.0 NaN\n",
      "\n",
      "DataFrame after dropping columns where all values are missing:\n",
      "     0    1    2\n",
      "0  1.0  6.5  3.0\n",
      "1  1.0  NaN  NaN\n",
      "2  NaN  NaN  NaN\n",
      "3  NaN  6.5  3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Create the initial DataFrame\n",
    "df = pd.DataFrame([\n",
    "    [1., 6.5, 3.],\n",
    "    [1., np.nan, np.nan],\n",
    "    [np.nan, np.nan, np.nan],\n",
    "    [np.nan, 6.5, 3.]\n",
    "])\n",
    "\n",
    "# Step 1: Add column indexed as 3 (i.e., 4th column) where all values are NA\n",
    "df[3] = np.nan\n",
    "\n",
    "print(\"DataFrame with new all-NA column:\")\n",
    "print(df)\n",
    "\n",
    "# Step 2: Drop the COLUMN where ALL values are NA\n",
    "df_cleaned = df.dropna(axis=1, how='all')\n",
    "\n",
    "print(\"\\nDataFrame after dropping columns where all values are missing:\")\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 7 </b></h3>\n",
    "<p>\n",
    "1. Create a df from random float numbers with shape (7, 3). </br>\n",
    "2. Assign NA to values at row index 0 to 3 inclusively and column index 1. </br>\n",
    "3. Assign NA to values at row index 0 to 1 inclusively and column index 2. </br>\n",
    "4. Fill NA values with 0. </br>\n",
    "5. Fill NA values with 1 for column 1, and fill NA values with 2 for column 2 using a dictionary. </br>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "           0         1         2\n",
      "0  1.557082  2.482382 -0.325690\n",
      "1 -1.333851 -0.480262 -1.085746\n",
      "2 -0.230311 -0.629006  0.982608\n",
      "3 -0.691996  1.161170 -0.484433\n",
      "4  1.188529  1.047122  0.202400\n",
      "5 -0.812344  1.046497 -0.027518\n",
      "6 -0.022648  0.063043  0.133190\n",
      "\n",
      "DataFrame with NaNs:\n",
      "           0         1         2\n",
      "0  1.557082       NaN       NaN\n",
      "1 -1.333851       NaN       NaN\n",
      "2 -0.230311       NaN  0.982608\n",
      "3 -0.691996       NaN -0.484433\n",
      "4  1.188529  1.047122  0.202400\n",
      "5 -0.812344  1.046497 -0.027518\n",
      "6 -0.022648  0.063043  0.133190\n",
      "\n",
      "NA filled with 0:\n",
      "           0         1         2\n",
      "0  1.557082  0.000000  0.000000\n",
      "1 -1.333851  0.000000  0.000000\n",
      "2 -0.230311  0.000000  0.982608\n",
      "3 -0.691996  0.000000 -0.484433\n",
      "4  1.188529  1.047122  0.202400\n",
      "5 -0.812344  1.046497 -0.027518\n",
      "6 -0.022648  0.063043  0.133190\n",
      "\n",
      "NA filled with {1:1, 2:2}:\n",
      "           0         1         2\n",
      "0  1.557082  1.000000  2.000000\n",
      "1 -1.333851  1.000000  2.000000\n",
      "2 -0.230311  1.000000  0.982608\n",
      "3 -0.691996  1.000000 -0.484433\n",
      "4  1.188529  1.047122  0.202400\n",
      "5 -0.812344  1.046497 -0.027518\n",
      "6 -0.022648  0.063043  0.133190\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create a DataFrame from random floats with shape (7, 3)\n",
    "df = pd.DataFrame(np.random.randn(7, 3))\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "# 2. Assign NA to values at row index 0 to 3 and column index 1\n",
    "df.loc[0:3, 1] = np.nan\n",
    "\n",
    "# 3. Assign NA to values at row index 0 to 1 and column index 2\n",
    "df.loc[0:1, 2] = np.nan\n",
    "\n",
    "print(\"\\nDataFrame with NaNs:\\n\", df)\n",
    "\n",
    "# 4. Fill all NA values with 0\n",
    "df_filled_all_zeros = df.fillna(0)\n",
    "print(\"\\nNA filled with 0:\\n\", df_filled_all_zeros)\n",
    "\n",
    "# 5. Fill NA values with 1 for column 1, and 2 for column 2 using a dictionary\n",
    "df_filled_custom = df.fillna({1: 1, 2: 2})\n",
    "print(\"\\nNA filled with {1:1, 2:2}:\\n\", df_filled_custom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 8 </b></h3>\n",
    "<p>\n",
    "1. Run below code and display the result. </br>\n",
    "2. Return a Boolean Series indicating whether a row is a duplicate. </br>\n",
    "3. Return a dataframe where the duplicated rows are dropped. </br>\n",
    "4. Return a dataframe where rows are dropped only if we have duplicates in column k2. </br>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     k1  k2\n",
      "0  one   1\n",
      "1  two   1\n",
      "2  one   2\n",
      "3  two   3\n",
      "4  one   3\n",
      "5  two   4\n",
      "6  two   4\n",
      "\n",
      "Boolean Series (duplicated rows):\n",
      " 0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "5    False\n",
      "6     True\n",
      "dtype: bool\n",
      "\n",
      "DataFrame without any duplicated rows:\n",
      "     k1  k2\n",
      "0  one   1\n",
      "1  two   1\n",
      "2  one   2\n",
      "3  two   3\n",
      "4  one   3\n",
      "5  two   4\n",
      "\n",
      "DataFrame without duplicates in column 'k2':\n",
      "     k1  k2\n",
      "0  one   1\n",
      "2  one   2\n",
      "3  two   3\n",
      "5  two   4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Create the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"k1\": [\"one\", \"two\"] * 3 + [\"two\"],\n",
    "    \"k2\": [1, 1, 2, 3, 3, 4, 4]\n",
    "})\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "# 2. Return a Boolean Series indicating whether a row is a duplicate\n",
    "duplicates_bool = df.duplicated()\n",
    "print(\"\\nBoolean Series (duplicated rows):\\n\", duplicates_bool)\n",
    "\n",
    "# 3. Return a DataFrame where the duplicated rows are dropped\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(\"\\nDataFrame without any duplicated rows:\\n\", df_no_duplicates)\n",
    "\n",
    "# 4. Return a DataFrame where rows are dropped only if we have duplicates in column 'k2'\n",
    "df_no_dup_k2 = df.drop_duplicates(subset='k2')\n",
    "print(\"\\nDataFrame without duplicates in column 'k2':\\n\", df_no_dup_k2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 9 </b></h3>\n",
    "<p>\n",
    "Add a new column called 'animal' to below dataframe by mapping meat_to_animal to it. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          food  ounces  animal\n",
      "0        bacon     4.0     pig\n",
      "1  pulled pork     3.0     pig\n",
      "2        bacon    12.0     pig\n",
      "3     pastrami     6.0     cow\n",
      "4  corned beef     7.5     cow\n",
      "5        bacon     8.0     pig\n",
      "6     pastrami     3.0     cow\n",
      "7    honey ham     5.0     pig\n",
      "8     nova lox     6.0  salmon\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"food\": [\"bacon\", \"pulled pork\", \"bacon\",\n",
    "             \"pastrami\", \"corned beef\", \"bacon\",\n",
    "             \"pastrami\", \"honey ham\", \"nova lox\"],\n",
    "    \"ounces\": [4, 3, 12, 6, 7.5, 8, 3, 5, 6]\n",
    "})\n",
    "\n",
    "meat_to_animal = {\n",
    "    \"bacon\": \"pig\",\n",
    "    \"pulled pork\": \"pig\",\n",
    "    \"pastrami\": \"cow\",\n",
    "    \"corned beef\": \"cow\",\n",
    "    \"honey ham\": \"pig\",\n",
    "    \"nova lox\": \"salmon\"\n",
    "}\n",
    "\n",
    "# Map the 'food' column using meat_to_animal dictionary\n",
    "df['animal'] = df['food'].map(meat_to_animal)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 10 </b></h3>\n",
    "<p>\n",
    "Tip: </br>\n",
    "- 'map' works element-wise on a Series; </br>\n",
    "- 'apply' works on a row / column basis of a DataFrame; </br>\n",
    "- 'applymap' works element-wise on a DataFrame; </br> </br>\n",
    "\n",
    "We could achieve the same result by mapping below function to the df - run below and analyze the result. </br>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_animal(x):\n",
    "    return meat_to_animal[x]\n",
    "\n",
    "df['animal'] = df.food.map(get_animal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 11 </b></h3>\n",
    "<p>\n",
    "As you've already seen, 'map' can be used to modify a subset of values in an object, but 'replace' provides a simpler and more flexible way to do so. </br>\n",
    "Given below Series replace value -999 with 0, and replace value -1000 with np.nan using replace() method.</br>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n",
      "1    0.0\n",
      "2    2.0\n",
      "3    0.0\n",
      "4    NaN\n",
      "5    3.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "s = pd.Series([1., -999., 2., -999., -1000., 3.])\n",
    "\n",
    "# Replace -999 with 0 and -1000 with np.nan\n",
    "s_replaced = s.replace({-999: 0, -1000: np.nan})\n",
    "\n",
    "print(s_replaced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 12</b></h3>\n",
    "<p>\n",
    "Like values in a Series, axis labels can be similarly transformed by a function or mapping of some form to produce new, differently labeled objects.  </br>\n",
    "1. Given below dataframe, create and map a custom function that capitalizes the index values. </br>\n",
    "2. Modify the dataframe in place by assigning the new index to it. </br>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          one  two  three  four\n",
      "OHIO        0    1      2     3\n",
      "COLORADO    4    5      6     7\n",
      "NEW YORK    8    9     10    11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(np.arange(12).reshape((3, 4)),\n",
    "                  index=[\"Ohio\", \"Colorado\", \"New York\"],\n",
    "                  columns=[\"one\", \"two\", \"three\", \"four\"])\n",
    "\n",
    "# 1. Create a custom function to capitalize index values\n",
    "def capitalize_index(name):\n",
    "    return name.upper()\n",
    "\n",
    "# 2. Modify the DataFrame in place\n",
    "df.index = df.index.map(capitalize_index)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 13 </b></h3>\n",
    "<p>\n",
    "If you want to create a transformed version of a dataset without modifying the original, a useful method is 'rename'. </br>\n",
    "Create a transformed version of the above dataframe by using a rename() method that capitalizes all column names. </br>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ONE  TWO  THREE  FOUR\n",
      "Ohio        0    1      2     3\n",
      "Colorado    4    5      6     7\n",
      "New York    8    9     10    11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Original DataFrame\n",
    "df = pd.DataFrame(np.arange(12).reshape((3, 4)),\n",
    "                  index=[\"Ohio\", \"Colorado\", \"New York\"],\n",
    "                  columns=[\"one\", \"two\", \"three\", \"four\"])\n",
    "\n",
    "# Create a transformed version with capitalized column names\n",
    "df_transformed = df.rename(columns=str.upper)\n",
    "\n",
    "print(df_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 14 </b></h3>\n",
    "<p>\n",
    "Now rename the above dataframe so that index \"COLORADO\" modifies to \"FOO\", and column \"two\" modifies to \"bar\". </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          one  bar  three  four\n",
      "OHIO        0    1      2     3\n",
      "FOO         4    5      6     7\n",
      "NEW YORK    8    9     10    11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Original DataFrame\n",
    "df = pd.DataFrame(np.arange(12).reshape((3, 4)),\n",
    "                  index=[\"Ohio\", \"Colorado\", \"New York\"],\n",
    "                  columns=[\"one\", \"two\", \"three\", \"four\"])\n",
    "\n",
    "# First capitalize the index and columns (as from previous steps)\n",
    "df.index = df.index.str.upper()\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Rename specific index and column\n",
    "df_renamed = df.rename(index={\"COLORADO\": \"FOO\"}, columns={\"two\": \"bar\"})\n",
    "\n",
    "print(df_renamed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 15 </b></h3>\n",
    "Regular Expressions.</br>\n",
    "Run below code and analyze the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foo', 'bar', 'baz', 'qux']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"foo    bar\\t baz  \\tqux\"\n",
    "re.split(r\"\\s+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3><b>Task 16</b></h3>\n",
    "When you call re.split(r\"\\s+\", text), the regular expression is first compiled, and then its split method is called on the passed text. </br>\n",
    "1. Now compile the regex yourself with re.compile, forming a reusable regex object.</br>\n",
    "2. Apply the compiled regex object to the 'text' string.</br>\n",
    "3. Now get a list of all patterns matching the compiled regex object, using the findall method</br></br>\n",
    "\n",
    "*Creating a regex object with re.compile is highly recommended if you intend to apply the same expression to many strings; doing so will save CPU cycles.</br>\n",
    "*'match' and 'search' are closely related to 'findall'. While 'findall' returns all matches in a string, 'search' returns only the first match. More rigidly, 'match' only matches at the beginning of the string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Result: ['foo', 'bar', 'baz', 'qux']\n",
      "Whitespace Matches: ['    ', '\\t ', '  \\t']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Original text\n",
    "text = \"foo    bar\\t baz  \\tqux\"\n",
    "\n",
    "# 1. Compile the regex\n",
    "pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "# 2. Apply the compiled regex to split the text\n",
    "split_result = pattern.split(text)\n",
    "print(\"Split Result:\", split_result)\n",
    "\n",
    "# 3. Get all whitespace patterns using findall\n",
    "matches = pattern.findall(text)\n",
    "print(\"Whitespace Matches:\", matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 17 </b></h3>\n",
    "<p>\n",
    "String Functions in pandas.</br>\n",
    "String and regular expression methods can be applied (passing a lambda or other function) to each value using data.map, but it will fail on the NA (null) values!</br>\n",
    "To cope with this, Series has array-oriented methods for string operations that skip over and propagate NA values. </br>\n",
    "These are accessed through Series’s 'str' attribute.</br>\n",
    "For example, we could check whether each email address has \"gmail\" in it with str.contains() as shown below. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dave     False\n",
      "Steve     True\n",
      "Rob       True\n",
      "Wes        NaN\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data with some email addresses and a missing value\n",
    "data = {\n",
    "    \"Dave\": \"dave@google.com\",\n",
    "    \"Steve\": \"steve@gmail.com\",\n",
    "    \"Rob\": \"rob@gmail.com\",\n",
    "    \"Wes\": np.nan\n",
    "}\n",
    "\n",
    "# Create a Series from the dictionary\n",
    "s = pd.Series(data)\n",
    "\n",
    "# Use the .str.contains() method to check if 'gmail' is in each string\n",
    "result = s.str.contains('gmail')\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 18 </b></h3>\n",
    "<p>\n",
    "Note that the result of this operation has an object dtype. </br>\n",
    "pandas has extension types that provide for specialized treatment of strings, integers, and Boolean data.</br>\n",
    "Run below code and pay attention to the dtype.</br>\n",
    "These 'string' arrays generally use much less memory and are frequently computationally more efficient for doing operations on large datasets. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dave     dave@google.com\n",
      "Steve    steve@gmail.com\n",
      "Rob        rob@gmail.com\n",
      "Wes                 <NA>\n",
      "dtype: string\n",
      "\n",
      "Dtype: string\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Original data with one missing value\n",
    "data = {\n",
    "    \"Dave\": \"dave@google.com\",\n",
    "    \"Steve\": \"steve@gmail.com\",\n",
    "    \"Rob\": \"rob@gmail.com\",\n",
    "    \"Wes\": np.nan\n",
    "}\n",
    "\n",
    "# Create Series\n",
    "s = pd.Series(data)\n",
    "\n",
    "# Convert to string extension type\n",
    "data_as_string_ext = s.astype('string')\n",
    "\n",
    "# Display the result and dtype\n",
    "print(data_as_string_ext)\n",
    "print(\"\\nDtype:\", data_as_string_ext.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 19 </b></h3>\n",
    "<p>\n",
    "Regular expressions can be used, too, along with any re options like IGNORECASE. </br>\n",
    "Analyze below pattern, run the code and pay attention to the syntax. </br>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dave     [(dave, google, com)]\n",
      "Steve    [(steve, gmail, com)]\n",
      "Rob        [(rob, gmail, com)]\n",
      "Wes                        NaN\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"Dave\": \"dave@google.com\",\n",
    "    \"Steve\": \"steve@gmail.com\",\n",
    "    \"Rob\": \"rob@gmail.com\",\n",
    "    \"Wes\": np.nan\n",
    "}\n",
    "\n",
    "s = pd.Series(data)\n",
    "\n",
    "# Regex pattern to extract parts of an email address\n",
    "pattern = r\"([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})\"\n",
    "\n",
    "# Apply the pattern using findall with IGNORECASE\n",
    "matches = s.str.findall(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "# Display results\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 20 </b></h3>\n",
    "<p>\n",
    "There are a couple of ways to do vectorized element retrieval. Either use str.get() or str.index().</br>\n",
    "Run below code and analyze the result. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dave     google\n",
      "Steve     gmail\n",
      "Rob       gmail\n",
      "Wes         NaN\n",
      "dtype: object\n",
      "Dave     dave@\n",
      "Steve    steve\n",
      "Rob      rob@g\n",
      "Wes        NaN\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "matches = s.str.findall(pattern, flags=re.IGNORECASE).str[0].str.get(1)\n",
    "print(matches)\n",
    "print(s.str[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Task 21 </b></h3>\n",
    "<p>\n",
    "The str.extract() method will return the captured groups of a regular expression as a DataFrame.</br>\n",
    "Run below code and analyze the result. </br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dave</th>\n",
       "      <td>dave</td>\n",
       "      <td>google</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steve</th>\n",
       "      <td>steve</td>\n",
       "      <td>gmail</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rob</th>\n",
       "      <td>rob</td>\n",
       "      <td>gmail</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wes</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0       1    2\n",
       "Dave    dave  google  com\n",
       "Steve  steve   gmail  com\n",
       "Rob      rob   gmail  com\n",
       "Wes      NaN     NaN  NaN"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.extract(pattern, flags = re.IGNORECASE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
